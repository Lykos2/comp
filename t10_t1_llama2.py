# -*- coding: utf-8 -*-
"""t10_t1_llama2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TWMGMalMXGtQa7rMZDBUPTA7JMqTOVB-
"""

!pip install datasets
!pip install evaluate
!pip install peft
!pip install wandb

from datasets import load_dataset, Dataset, concatenate_datasets, DatasetDict
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler, DataCollatorWithPadding, Trainer, TrainingArguments
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import DataLoader
from torch.optim import AdamW
import torch
import torch.nn.functional as F
from tqdm.auto import tqdm
import evaluate
import pandas as pd
from peft import get_peft_model, LoraConfig, TaskType
import numpy as np
from collections import Counter
import gc

# hyperparameters
lr = 1e-4
num_epochs = 10 # 4 (or 7 epochs seems good)
batch_size = 16
llama_checkpoint = "meta-llama/Llama-2-7b-hf"
# MAX_LEN = 512  # mandotory for roberta optional for others

llama_model = AutoModelForSequenceClassification.from_pretrained(
    pretrained_model_name_or_path=llama_checkpoint,
    num_labels=8,
    # device_map="auto", #should try without this
    offload_folder="offload",# try without this
    trust_remote_code=True
)
#For Llama 2, we have to add the padding token id as it is not defined by default.
llama_model.config.pad_token_id = llama_model.config.eos_token_id

llama_tokenizer = AutoTokenizer.from_pretrained(llama_checkpoint, add_prefix_spac=True)
llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id
llama_tokenizer.pad_token = llama_tokenizer.eos_token

train_data_files = {
    "train": "MaSaC_train_erc_task1.json",
    "validation": "MaSaC_validation_erc_task1.json"
}

train_dataset = load_dataset("json", data_files=train_data_files)
train_dataset = train_dataset.remove_columns("episode")

labels = ['anger', 'neutral', 'contempt', 'sadness', 'fear', 'disgust', 'joy', 'surprise']
label_encoder = LabelEncoder()
label_encoder.fit(labels)

combined_dataset = {}
for dataset_type in train_dataset:
    for record in train_dataset[dataset_type]:
        encoded_label = label_encoder.transform(record["emotions"])
        record["emotions"] = encoded_label
        if combined_dataset.get(dataset_type) is None:
            combined_dataset[dataset_type] = Dataset.from_dict(record)
        else:
            combined_dataset[dataset_type] = concatenate_datasets([combined_dataset[dataset_type], Dataset.from_dict(record)])

dataset_dict = DatasetDict(combined_dataset)


dataset_dict['train'].to_pandas().info()
dataset_dict['validation'].to_pandas().info()

max_char = dataset_dict['train'].to_pandas()['utterances'].str.len().max()
max_words = dataset_dict['train'].to_pandas()['utterances'].str.split().str.len().max()

"""code above common for all 3 models"""

label_counts = Counter(dataset_dict["train"]["emotions"])
num_classes = 8
num_samples = len(dataset_dict["train"])
class_weights = [num_samples / (num_classes * label_counts[i]) for i in range(num_classes)]

col_to_delete = ["speakers", "utterances"]

def llama_preporcessing_function(exampmle):
    return llama_tokenizer(exampmle["utterances"], truncation=True)

llama_tokenized_datasets = dataset_dict.map(llama_preporcessing_function, batched=True, remove_columns=col_to_delete)
llama_tokenized_datasets = llama_tokenized_datasets.rename_column("emotions", "label") # check the label val for each model individually
llama_tokenized_datasets.set_format("torch")

llama_data_collator = DataCollatorWithPadding(tokenizer=llama_tokenizer)

# Lora setup for llama classifier
llama_peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS, r=16, lora_alpha=16, lora_dropout=0.5, bias="none",
    target_modules=[
        "q_proj",
        "v_proj"
    ]
)
llama_model = get_peft_model(llama_model, llama_peft_config)
llama_model.print_trainable_parameters()

def compute_metrics(eval_pred):
    precision_metric = evaluate.load("precision")
    recall_metric = evaluate.load("recall")
    f1_metric = evaluate.load("f1")
    accuracy_metric = evaluate.load("accuracy")

    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')["precision"]
    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')["recall"]
    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')["f1"]
    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)["accuracy"]
    return {"precision": precision, "recall": recall, "f1-score": f1, 'accuracy': accuracy}

torch.cuda.empty_cache()
gc.collect()

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

# not that good at all, wasnt representative
class WeightedCELossTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        # Get model's predictions
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # Compute custom loss
        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, device=model.device, dtype=logits.dtype))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

# can make this custom loss function https://chat.openai.com/share/5c3b8a5b-7cbb-4bed-bcff-7470773cfc26
class WeightedCEWithF1LossTrainer(Trainer):

    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # class_weights = torch.tensor(class_weights, device=model.device, dtype=logits.dtype)
        loss = self.weighted_ce_f1_loss(labels, logits, torch.tensor(class_weights, device=model.device, dtype=logits.dtype))

        return (loss, outputs) if return_outputs else loss

    @staticmethod
    def weighted_ce_f1_loss(y_true, logits, epsilon=1e-7):
        # Convert labels to one-hot encoding
        y_true_one_hot = F.one_hot(y_true, num_classes=logits.size(-1)).float()

        # Apply softmax to logits to get probabilities
        y_pred = F.softmax(logits, dim=-1)

        # Calculate Cross Entropy Loss without reduction
        ce = F.cross_entropy(logits, y_true, reduction='none')
        # Ensure ce is [batch_size, 1] for broadcasting
        ce = ce.view(-1, 1)

        # Calculate components of F1 score for each sample and each class
        tp = (y_true_one_hot * y_pred)
        fp = ((1 - y_true_one_hot) * y_pred)
        fn = (y_true_one_hot * (1 - y_pred))

        # Sum tp, fp, fn over all samples for each class
        tp_sum = tp.sum(dim=0)
        fp_sum = fp.sum(dim=0)
        fn_sum = fn.sum(dim=0)

        # Calculate Precision and Recall for each class
        precision = tp_sum / (tp_sum + fp_sum + epsilon)
        recall = tp_sum / (tp_sum + fn_sum + epsilon)

        # Calculate F1 Score for each class
        f1 = 2 * precision * recall / (precision + recall + epsilon)
        f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1), f1)

        # Calculate mean F1 score across all classes
        f1_mean = f1.mean()
        # Ensure f1_mean is broadcastable with ce
        f1_mean = f1_mean.view(1, -1)

        # Calculate Weighted Loss
        weighted_loss = ce * (1 - f1_mean)
        return torch.mean(weighted_loss)

class WeightedF1ApproxLossTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")

        # Cross-entropy part
        ce_loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, device=model.device, dtype=logits.dtype))
        ce_loss = ce_loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))

        # F1-approximating part
        # Softmax to get probabilities
        probs = F.softmax(logits, dim=-1)
        # One-hot encode labels
        true_labels = F.one_hot(labels, num_classes=self.model.config.num_labels).to(dtype=probs.dtype)

        # Calculate precision and recall per class
        tp = torch.sum(probs * true_labels, axis=0)
        fp = torch.sum(probs * (1 - true_labels), axis=0)
        fn = torch.sum((1 - probs) * true_labels, axis=0)

        precision = tp / (tp + fp + 1e-8)
        recall = tp / (tp + fn + 1e-8)

        # Use harmonic mean of precision and recall as a proxy for F1
        f1_proxy = 2 * (precision * recall) / (precision + recall + 1e-8)
        f1_proxy_loss = 1 - torch.mean(f1_proxy)  # Loss is 1 - mean F1 proxy

        # Combine losses
        # You might need to tune the balance coefficient
        balance_coefficient = 0.5
        loss = (1 - balance_coefficient) * ce_loss + balance_coefficient * f1_proxy_loss

        return (loss, outputs) if return_outputs else loss

llama_model = llama_model.cuda()

training_args = TrainingArguments(
    output_dir="llama-lora-token-classification",
    learning_rate=lr,
    lr_scheduler_type= "constant",
    warmup_ratio= 0.1,
    max_grad_norm= 0.3,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.001,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    report_to="wandb",
    fp16=True,
    gradient_checkpointing=True,
)

llama_trainer = WeightedCEWithF1LossTrainer(
    model=llama_model,
    args=training_args,
    train_dataset=llama_tokenized_datasets['train'],
    eval_dataset=llama_tokenized_datasets["validation"],
    data_collator=llama_data_collator,
    compute_metrics=compute_metrics
)

torch.cuda.empty_cache()
gc.collect()

llama_trainer.train()

"""Eval"""

test_data_files = {
    "test": "MaSaC_test_erc_task1.json",
}

dataset = load_dataset("json", data_files=test_data_files)
dataset = dataset.remove_columns("episode")

combined_dataset = {}
for dataset_type in dataset:
    for record in dataset[dataset_type]:
        if combined_dataset.get(dataset_type) is None:
            combined_dataset[dataset_type] = Dataset.from_dict(record)
        else:
            combined_dataset[dataset_type] = concatenate_datasets([combined_dataset[dataset_type], Dataset.from_dict(record)])

dataset_dict_test = DatasetDict(combined_dataset)
print(dataset_dict_test)

llama_tokenized_datasets = dataset_dict_test.map(llama_preporcessing_function, batched=True, remove_columns=col_to_delete)
llama_tokenized_datasets.set_format("torch")

print(llama_tokenized_datasets)

test_dataloader = DataLoader(llama_tokenized_datasets["test"], batch_size=15, collate_fn=llama_data_collator)

emotions = []

llama_model.eval()
for batch in test_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = llama_model(**batch)
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    predictions = predictions.cpu().tolist()
    decoded_label = label_encoder.inverse_transform(predictions).tolist()
    emotions = emotions + decoded_label

print(emotions, len(emotions))

file_path = 'answer_t1_llama_0_43.txt'

with open(file_path, 'w') as file:
    for item in emotions:
        file.write(item + '\n')

